/*
 * Here resides the implementation of the activation function. Initially there
 * will only be support of the ReLu function as that is featured in the base
 * implementation in the PyTorch implementation. But if I revisit this project
 * in the future I will be sure to add support for other activation functions
 * such as TanH or Sigmoid*/

void __global__ ReLu() {}
